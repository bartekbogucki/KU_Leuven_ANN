{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i1KfYGFRRWP"
      },
      "source": [
        "## Artificial Neural Networks and Deep Learning  \n",
        "##Assignment 3.3 - Self-attention and Transformers\n",
        "\n",
        "Prof. Dr. Ir. Johan A. K. Suykens     \n",
        "\n",
        "In this file, we first understand the self-attention mechanism by implementing it both with ``NumPy`` and ``PyTorch``.\n",
        "Then, we implement a 6-layer Vision Transformer (ViT) and train it on the MNIST dataset.\n",
        "\n",
        "All training will be conducted on a single T4 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlTJbgaaRTct"
      },
      "outputs": [],
      "source": [
        "# Please first load your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu6w5GLkRezN"
      },
      "outputs": [],
      "source": [
        "# Please go to Edit > Notebook settings > Hardware accelerator > choose \"T4 GPU\"\n",
        "# Now check if you have loaded the GPU successfully\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-sUz1A9SzVH"
      },
      "source": [
        "# Self-attention Mechanism\n",
        "Self-attention is the core mechanism in Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ol1XZtiPpk"
      },
      "source": [
        "## Self-attention with NumPy\n",
        "To have a better understanding of it, we first manually implement self-attention mechanism with ``numpy``. You can check the dimension of each variable during the matrix computation.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgWIgp51RgC3",
        "outputId": "d6509cb3-fc52-47bf-c0b6-279f387d2ba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The attention outputs are\n",
            " [[ 0.5220374   1.86564986 -0.80890358 ...  1.10937697 -3.07645862\n",
            "  -3.57747067]\n",
            " [ 0.5220374   1.86564986 -0.80890358 ...  1.10937697 -3.07645862\n",
            "  -3.57747067]\n",
            " [ 0.5220374   1.86564986 -0.80890358 ...  1.10937697 -3.07645862\n",
            "  -3.57747067]\n",
            " ...\n",
            " [ 0.5220374   1.86564986 -0.80890358 ...  1.10937697 -3.07645862\n",
            "  -3.57747067]\n",
            " [ 0.5220374   1.86564986 -0.80890358 ...  1.10937697 -3.07645862\n",
            "  -3.57747067]\n",
            " [ 0.5220374   1.86564986 -0.80890358 ...  1.10937697 -3.07645862\n",
            "  -3.57747067]]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "# I. Define the input data X\n",
        "# X consists out of 32 samples, each sample has dimensionality 256\n",
        "n = 32\n",
        "d = 256\n",
        "X = randn(n, d) # (32, 256)\n",
        "\n",
        "# II. Generate the projection weights\n",
        "Wq = randn(d, d) #(256, 256)\n",
        "Wk = randn(d, d)\n",
        "Wv = randn(d, d)\n",
        "\n",
        "# III. Project X to find its query, keys and values vectors\n",
        "Q = np.dot(X, Wq) # (32, 256)\n",
        "K = np.dot(X, Wk)\n",
        "V = np.dot(X, Wv)\n",
        "\n",
        "# IV. Compute the self-attention score, denoted by A\n",
        "# A = softmax(QK^T / \\sqrt{d})\n",
        "# Define the softmax function\n",
        "def softmax(z):\n",
        "    z = np.clip(z, 100, -100) # clip in case softmax explodes\n",
        "    tmp = np.exp(z)\n",
        "    res = np.exp(z) / np.sum(tmp, axis=1)\n",
        "    return res\n",
        "\n",
        "A = softmax(np.dot(Q, K.transpose())/math.sqrt(d)) #(32, 32)\n",
        "\n",
        "# V. Compute the self-attention output\n",
        "# outputs = A * V\n",
        "outputs = np.dot(A, V) #(32, 256)\n",
        "\n",
        "print(\"The attention outputs are\\n {}\".format(outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iozM1k4khO0B"
      },
      "source": [
        "## Self-attention with PyTorch\n",
        "Now, we implement self-attention with ``PyTorch``, which is commonly used when building Transformers.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qng07v8xdaPj",
        "outputId": "48bc13bd-6527-4837-e4c3-dde994fc9a97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape:torch.Size([32, 20, 128]) \n",
            " Q.shape:torch.Size([32, 20, 64]) \n",
            " K.shape:torch.Size([32, 20, 64]) \n",
            " V.shape:torch.Size([32, 20, 32])\n",
            "attention matrix:  torch.Size([32, 20, 20])\n",
            "attention outputs:  torch.Size([32, 20, 32])\n",
            "tensor([[[ 1.2748e-01,  3.2062e-01, -9.0824e-02,  ...,  4.8949e-02,\n",
            "          -8.9586e-02,  5.2573e-02],\n",
            "         [ 6.9496e-02,  2.3644e-01, -1.2103e-01,  ..., -1.9161e-02,\n",
            "          -1.0948e-01,  9.4250e-02],\n",
            "         [ 1.3117e-01,  1.9833e-01, -1.0276e-01,  ...,  8.9701e-04,\n",
            "          -4.4238e-02,  7.4584e-02],\n",
            "         ...,\n",
            "         [ 1.0973e-01,  2.5184e-01, -8.4412e-02,  ...,  7.0247e-03,\n",
            "           6.8269e-03,  5.5296e-02],\n",
            "         [ 5.0689e-02,  2.2151e-01, -8.5633e-02,  ..., -5.7543e-03,\n",
            "          -4.0312e-02,  9.8725e-02],\n",
            "         [ 1.8154e-01,  1.9808e-01, -4.8706e-02,  ..., -4.9543e-02,\n",
            "          -9.0759e-03,  7.9488e-02]],\n",
            "\n",
            "        [[-2.6528e-02, -1.7695e-01,  1.5042e-01,  ...,  5.2018e-02,\n",
            "           5.7282e-02, -1.5907e-02],\n",
            "         [-7.0393e-02, -2.0748e-01,  1.6168e-01,  ...,  8.1489e-02,\n",
            "           8.5149e-03,  6.3408e-03],\n",
            "         [-7.3200e-02, -2.2809e-01,  2.1224e-01,  ...,  5.4141e-02,\n",
            "          -8.7922e-02,  2.1552e-02],\n",
            "         ...,\n",
            "         [-1.4219e-01, -2.3983e-01,  2.4411e-01,  ...,  1.2306e-01,\n",
            "          -3.1024e-02, -5.0904e-02],\n",
            "         [-7.1217e-02, -2.0147e-01,  1.6202e-01,  ...,  8.9797e-02,\n",
            "           2.3933e-02,  1.8349e-02],\n",
            "         [-3.1027e-02, -1.8699e-01,  2.2880e-01,  ...,  1.0438e-01,\n",
            "          -1.1353e-01, -5.9131e-03]],\n",
            "\n",
            "        [[-3.0345e-01,  2.3170e-01, -2.9113e-02,  ..., -1.5695e-01,\n",
            "           8.4283e-03, -3.2648e-02],\n",
            "         [-2.8215e-01,  2.1661e-01, -3.9841e-02,  ..., -1.5267e-01,\n",
            "           6.8903e-03,  2.9534e-02],\n",
            "         [-3.0662e-01,  1.4106e-01, -7.1046e-02,  ..., -1.7190e-01,\n",
            "           1.9878e-03, -1.0634e-04],\n",
            "         ...,\n",
            "         [-3.0691e-01,  1.5057e-01, -5.2751e-02,  ..., -1.7035e-01,\n",
            "           8.0856e-02,  4.1445e-02],\n",
            "         [-3.8412e-01,  1.9252e-01, -9.3504e-02,  ..., -1.8046e-01,\n",
            "          -7.9398e-03,  4.5705e-02],\n",
            "         [-3.1642e-01,  2.0569e-01, -4.2027e-02,  ..., -1.1882e-01,\n",
            "           3.7411e-02,  2.1067e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-8.4079e-02, -2.3433e-01,  6.5098e-03,  ...,  8.2890e-02,\n",
            "           9.8108e-02, -5.7926e-03],\n",
            "         [-7.2868e-02, -1.5976e-01,  2.9119e-02,  ...,  6.0007e-02,\n",
            "           1.1762e-01, -2.0596e-02],\n",
            "         [-9.6419e-02, -2.2389e-01,  1.5330e-02,  ...,  8.9976e-02,\n",
            "           5.6540e-02, -1.1020e-02],\n",
            "         ...,\n",
            "         [-5.3457e-02, -1.6251e-01,  3.3974e-02,  ..., -1.1638e-02,\n",
            "           1.0079e-01, -2.1852e-01],\n",
            "         [-6.8591e-02, -2.4292e-01,  9.7649e-02,  ...,  3.3627e-02,\n",
            "           1.7805e-01, -1.2719e-02],\n",
            "         [-9.4819e-02, -1.7077e-01,  7.1747e-03,  ...,  4.0348e-02,\n",
            "           9.7518e-02, -2.5347e-02]],\n",
            "\n",
            "        [[-9.9792e-02, -1.7943e-02, -6.9075e-02,  ...,  4.2263e-02,\n",
            "          -1.0519e-01,  1.3938e-01],\n",
            "         [-1.0636e-01,  3.5283e-02,  5.4539e-03,  ...,  1.0909e-01,\n",
            "          -8.3902e-02,  5.2285e-02],\n",
            "         [ 8.0992e-02,  1.0227e-02,  4.5836e-02,  ...,  1.9670e-01,\n",
            "          -9.3913e-02,  6.6408e-02],\n",
            "         ...,\n",
            "         [-4.9718e-02, -6.5059e-04, -2.3544e-02,  ...,  8.7936e-02,\n",
            "          -9.4707e-02,  1.0093e-01],\n",
            "         [-6.6379e-02, -7.9469e-02,  9.7339e-03,  ...,  9.3014e-02,\n",
            "          -9.4449e-02,  7.4799e-02],\n",
            "         [-5.2845e-02,  7.5402e-02, -6.8490e-02,  ...,  1.0166e-01,\n",
            "          -8.6809e-02,  8.3978e-02]],\n",
            "\n",
            "        [[-2.7668e-01, -2.7363e-01,  2.1228e-01,  ..., -8.4182e-03,\n",
            "           2.2989e-01,  1.7661e-01],\n",
            "         [-2.4895e-01, -1.4018e-01,  1.5863e-01,  ..., -1.2735e-01,\n",
            "           1.9244e-01,  1.2758e-01],\n",
            "         [-2.6836e-01, -2.6428e-01,  2.0991e-01,  ...,  3.0040e-02,\n",
            "           1.4305e-01,  1.8320e-01],\n",
            "         ...,\n",
            "         [-2.3722e-01, -2.9385e-01,  2.0861e-01,  ..., -3.5827e-02,\n",
            "           1.2893e-01,  2.1908e-01],\n",
            "         [-2.4619e-01, -2.9602e-01,  1.3680e-01,  ..., -3.4631e-02,\n",
            "           1.9687e-01,  2.5820e-01],\n",
            "         [-3.0692e-01, -1.6807e-01,  1.8221e-01,  ..., -2.6145e-02,\n",
            "           2.2744e-01,  1.7445e-01]]], grad_fn=<BmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, dim_input, dim_q, dim_v):\n",
        "        '''\n",
        "        dim_input: the dimension of each sample\n",
        "        dim_q: dimension of Q matrix, should be equal to dim_k\n",
        "        dim_v: dimension of V matrix, also the  dimension of the attention output\n",
        "        '''\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_q = dim_q\n",
        "        self.dim_k = dim_q\n",
        "        self.dim_v = dim_v\n",
        "\n",
        "        # Define the linear projection\n",
        "        self.linear_q = nn.Linear(self.dim_input, self.dim_q, bias=False)\n",
        "        self.linear_k = nn.Linear(self.dim_input, self.dim_k, bias=False)\n",
        "        self.linear_v = nn.Linear(self.dim_input, self.dim_v, bias=False)\n",
        "        self._norm_fact = 1 / math.sqrt(self.dim_k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_q = x.shape\n",
        "\n",
        "        q = self.linear_q(x) # (batchsize, seq_len, dim_q)\n",
        "        k = self.linear_k(x) # (batchsize, seq_len, dim_k)\n",
        "        v = self.linear_v(x) # (batchsize, seq_len, dim_v)\n",
        "        print(f'x.shape:{x.shape} \\n Q.shape:{q.shape} \\n K.shape:{k.shape} \\n V.shape:{v.shape}')\n",
        "\n",
        "        dist = torch.bmm(q, k.transpose(1,2)) * self._norm_fact\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "        print('attention matrix: ', dist.shape)\n",
        "\n",
        "        outputs = torch.bmm(dist, v)\n",
        "        print('attention outputs: ', outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "batch_size = 32 # number of samples in a batch\n",
        "dim_input = 128 # dimension of each item in the sample sequence\n",
        "seq_len = 20 # sequence length for each sample\n",
        "x = torch.randn(batch_size, seq_len, dim_input)\n",
        "self_attention = SelfAttention(dim_input, dim_q = 64, dim_v = 32)\n",
        "\n",
        "attention = self_attention(x)\n",
        "\n",
        "print(attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZaAFL8MS2Ng"
      },
      "source": [
        "# Transformers\n",
        "In this section, we implement a 6-layer Vision Transformer (ViT) and trained it on the MNIST dataset.\n",
        "We consider the classification tasks.\n",
        "First, we load the MNIST dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ-eIaeZjWjL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def get_mnist_loader(batch_size=100, shuffle=True):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_dataset = MNIST(root='../data',\n",
        "                          train=True,\n",
        "                          transform=torchvision.transforms.ToTensor(),\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='../data',\n",
        "                         train=False,\n",
        "                         transform=torchvision.transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=shuffle)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C06IoPIjePg",
        "outputId": "524cd1e3-72c7-43cc-ad25-66689755983a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "# This package is needed to build the transformer\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx9eZrMpmA2z"
      },
      "source": [
        "## Build ViT from scratch\n",
        "Recall that each Transformer block include 2 modules: the self-attention module, the feedforward module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr6d7IWfjpxY"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(), # Gaussian Error Linear Units is another type of activation function\n",
        "            nn.Linear(mlp_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTawNC64mhBO"
      },
      "source": [
        "## Training and test function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKJ4tjCjjycH"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vph2CrNxj6ZZ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    # We do not need to remember the gradients when testing\n",
        "    # This will help reduce memory\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRYys50km0-E"
      },
      "source": [
        "## Let's start training!\n",
        "Here, you can change the ViT structure by changing the hyper-parametrs inside ``ViT`` function.\n",
        "The default settings are with 6 layers, 8 heads for the multi-head attention mechanism and embedding dimension of 64.\n",
        "You can also increase the number of epochs to obtain better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVLJLLDuj7yQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# You can change the architecture here\n",
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
        "model = model.cuda()\n",
        "# We also print the network architecture\n",
        "model\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vlt3tk-MkDB9",
        "outputId": "e46d5256-1662-42f9-da3e-c357db534f30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 515kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.61MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.4613\n",
            "[12800/60000 ( 21%)]  Loss: 1.0506\n",
            "[25600/60000 ( 43%)]  Loss: 0.4662\n",
            "[38400/60000 ( 64%)]  Loss: 0.3025\n",
            "[51200/60000 ( 85%)]  Loss: 0.1466\n",
            "\n",
            "Average test loss: 0.2259  Accuracy: 9281/10000 (92.81%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1706\n",
            "[12800/60000 ( 21%)]  Loss: 0.3368\n",
            "[25600/60000 ( 43%)]  Loss: 0.0974\n",
            "[38400/60000 ( 64%)]  Loss: 0.2440\n",
            "[51200/60000 ( 85%)]  Loss: 0.0976\n",
            "\n",
            "Average test loss: 0.1228  Accuracy: 9602/10000 (96.02%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1991\n",
            "[12800/60000 ( 21%)]  Loss: 0.1714\n",
            "[25600/60000 ( 43%)]  Loss: 0.1239\n",
            "[38400/60000 ( 64%)]  Loss: 0.0754\n",
            "[51200/60000 ( 85%)]  Loss: 0.0972\n",
            "\n",
            "Average test loss: 0.0999  Accuracy: 9698/10000 (96.98%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1090\n",
            "[12800/60000 ( 21%)]  Loss: 0.0965\n",
            "[25600/60000 ( 43%)]  Loss: 0.1016\n",
            "[38400/60000 ( 64%)]  Loss: 0.1047\n",
            "[51200/60000 ( 85%)]  Loss: 0.1229\n",
            "\n",
            "Average test loss: 0.0875  Accuracy: 9718/10000 (97.18%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0674\n",
            "[12800/60000 ( 21%)]  Loss: 0.0566\n",
            "[25600/60000 ( 43%)]  Loss: 0.0749\n",
            "[38400/60000 ( 64%)]  Loss: 0.0356\n",
            "[51200/60000 ( 85%)]  Loss: 0.0579\n",
            "\n",
            "Average test loss: 0.0746  Accuracy: 9770/10000 (97.70%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0435\n",
            "[12800/60000 ( 21%)]  Loss: 0.0335\n",
            "[25600/60000 ( 43%)]  Loss: 0.0074\n",
            "[38400/60000 ( 64%)]  Loss: 0.0250\n",
            "[51200/60000 ( 85%)]  Loss: 0.0616\n",
            "\n",
            "Average test loss: 0.0786  Accuracy: 9756/10000 (97.56%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0656\n",
            "[12800/60000 ( 21%)]  Loss: 0.0242\n",
            "[25600/60000 ( 43%)]  Loss: 0.0502\n",
            "[38400/60000 ( 64%)]  Loss: 0.0504\n",
            "[51200/60000 ( 85%)]  Loss: 0.0369\n",
            "\n",
            "Average test loss: 0.0666  Accuracy: 9795/10000 (97.95%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0212\n",
            "[12800/60000 ( 21%)]  Loss: 0.0058\n",
            "[25600/60000 ( 43%)]  Loss: 0.1224\n",
            "[38400/60000 ( 64%)]  Loss: 0.0148\n",
            "[51200/60000 ( 85%)]  Loss: 0.0341\n",
            "\n",
            "Average test loss: 0.0640  Accuracy: 9805/10000 (98.05%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0189\n",
            "[12800/60000 ( 21%)]  Loss: 0.0111\n",
            "[25600/60000 ( 43%)]  Loss: 0.0162\n",
            "[38400/60000 ( 64%)]  Loss: 0.0253\n",
            "[51200/60000 ( 85%)]  Loss: 0.0229\n",
            "\n",
            "Average test loss: 0.0681  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0348\n",
            "[12800/60000 ( 21%)]  Loss: 0.0135\n",
            "[25600/60000 ( 43%)]  Loss: 0.0095\n",
            "[38400/60000 ( 64%)]  Loss: 0.0293\n",
            "[51200/60000 ( 85%)]  Loss: 0.0755\n",
            "\n",
            "Average test loss: 0.0611  Accuracy: 9832/10000 (98.32%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0161\n",
            "[12800/60000 ( 21%)]  Loss: 0.0091\n",
            "[25600/60000 ( 43%)]  Loss: 0.0014\n",
            "[38400/60000 ( 64%)]  Loss: 0.0008\n",
            "[51200/60000 ( 85%)]  Loss: 0.0058\n",
            "\n",
            "Average test loss: 0.0671  Accuracy: 9804/10000 (98.04%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0066\n",
            "[12800/60000 ( 21%)]  Loss: 0.0087\n",
            "[25600/60000 ( 43%)]  Loss: 0.0778\n",
            "[38400/60000 ( 64%)]  Loss: 0.0055\n",
            "[51200/60000 ( 85%)]  Loss: 0.0067\n",
            "\n",
            "Average test loss: 0.0595  Accuracy: 9829/10000 (98.29%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0186\n",
            "[12800/60000 ( 21%)]  Loss: 0.0041\n",
            "[25600/60000 ( 43%)]  Loss: 0.0149\n",
            "[38400/60000 ( 64%)]  Loss: 0.0011\n",
            "[51200/60000 ( 85%)]  Loss: 0.0824\n",
            "\n",
            "Average test loss: 0.0731  Accuracy: 9804/10000 (98.04%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0125\n",
            "[12800/60000 ( 21%)]  Loss: 0.0320\n",
            "[25600/60000 ( 43%)]  Loss: 0.0039\n",
            "[38400/60000 ( 64%)]  Loss: 0.0222\n",
            "[51200/60000 ( 85%)]  Loss: 0.0087\n",
            "\n",
            "Average test loss: 0.0815  Accuracy: 9790/10000 (97.90%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0205\n",
            "[12800/60000 ( 21%)]  Loss: 0.0233\n",
            "[25600/60000 ( 43%)]  Loss: 0.0020\n",
            "[38400/60000 ( 64%)]  Loss: 0.0004\n",
            "[51200/60000 ( 85%)]  Loss: 0.0015\n",
            "\n",
            "Average test loss: 0.0603  Accuracy: 9840/10000 (98.40%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0019\n",
            "[12800/60000 ( 21%)]  Loss: 0.0004\n",
            "[25600/60000 ( 43%)]  Loss: 0.0128\n",
            "[38400/60000 ( 64%)]  Loss: 0.0018\n",
            "[51200/60000 ( 85%)]  Loss: 0.0091\n",
            "\n",
            "Average test loss: 0.0711  Accuracy: 9828/10000 (98.28%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0123\n",
            "[12800/60000 ( 21%)]  Loss: 0.0083\n",
            "[25600/60000 ( 43%)]  Loss: 0.0153\n",
            "[38400/60000 ( 64%)]  Loss: 0.0139\n",
            "[51200/60000 ( 85%)]  Loss: 0.0133\n",
            "\n",
            "Average test loss: 0.0655  Accuracy: 9848/10000 (98.48%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0185\n",
            "[12800/60000 ( 21%)]  Loss: 0.0036\n",
            "[25600/60000 ( 43%)]  Loss: 0.0014\n",
            "[38400/60000 ( 64%)]  Loss: 0.0002\n",
            "[51200/60000 ( 85%)]  Loss: 0.0077\n",
            "\n",
            "Average test loss: 0.0755  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0232\n",
            "[12800/60000 ( 21%)]  Loss: 0.0030\n",
            "[25600/60000 ( 43%)]  Loss: 0.0197\n",
            "[38400/60000 ( 64%)]  Loss: 0.0019\n",
            "[51200/60000 ( 85%)]  Loss: 0.0008\n",
            "\n",
            "Average test loss: 0.0721  Accuracy: 9844/10000 (98.44%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0033\n",
            "[12800/60000 ( 21%)]  Loss: 0.0081\n",
            "[25600/60000 ( 43%)]  Loss: 0.0004\n",
            "[38400/60000 ( 64%)]  Loss: 0.0103\n",
            "[51200/60000 ( 85%)]  Loss: 0.0011\n",
            "\n",
            "Average test loss: 0.0683  Accuracy: 9849/10000 (98.49%)\n",
            "\n",
            "Execution time: 270.95 seconds\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "# Gradually reduce the learning rate while training\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "    scheduler.step()\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "\n",
        "# ————————————————————————————————————————————————————————————\n",
        "# 1) wrap your existing loop in a function\n",
        "# ————————————————————————————————————————————————————————————\n",
        "def train_vit_model(params,\n",
        "                    n_epochs=20,\n",
        "                    batch_size=128,\n",
        "                    lr=1e-3,\n",
        "                    device='cuda'):\n",
        "    # instantiate ViT exactly as in your baseline\n",
        "    model = ViT(\n",
        "        image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "        dim=params['dim'], depth=params['depth'],\n",
        "        heads=params['heads'], mlp_dim=params['mlp_dim']\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "    train_loss_history, test_loss_history = [], []\n",
        "    train_loader, test_loader = get_mnist_loader(batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "        train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "        evaluate(model, test_loader, test_loss_history)\n",
        "        scheduler.step()\n",
        "\n",
        "# ————————————————————————————————————————————————————————————\n",
        "# 2) define your grid of architectures (including your baseline)\n",
        "# ————————————————————————————————————————————————————————————\n",
        "param_grid = OrderedDict([\n",
        "    (\"baseline\", {\"dim\": 64,  \"depth\": 6, \"heads\": 8,  \"mlp_dim\": 128}),\n",
        "    (\"smaller\",  {\"dim\": 32,  \"depth\": 4, \"heads\": 4,  \"mlp_dim\": 64 }),\n",
        "    (\"wider\",    {\"dim\": 128, \"depth\": 6, \"heads\": 8,  \"mlp_dim\": 256}),\n",
        "    (\"deeper\",   {\"dim\": 64,  \"depth\": 8, \"heads\": 8,  \"mlp_dim\": 128}),\n",
        "    (\"combo\",    {\"dim\": 128, \"depth\": 8, \"heads\": 16, \"mlp_dim\": 512}),\n",
        "])\n",
        "\n",
        "# ————————————————————————————————————————————————————————————\n",
        "# 3) run all experiments and collect results\n",
        "# ————————————————————————————————————————————————————————————\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "results = OrderedDict()\n",
        "\n",
        "for name, params in param_grid.items():\n",
        "    print(f\"=== Running ViT variant: {name} ===\")\n",
        "    train_vit_model(params,\n",
        "                          n_epochs=N_EPOCHS,\n",
        "                          batch_size=128,\n",
        "                          lr=0.001,\n",
        "                          device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnmpBFp0NbJ0",
        "outputId": "5338daf2-a0f6-4532-bf93-240697d69090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Running ViT variant: baseline ===\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3265\n",
            "[12800/60000 ( 21%)]  Loss: 1.1083\n",
            "[25600/60000 ( 43%)]  Loss: 0.3552\n",
            "[38400/60000 ( 64%)]  Loss: 0.3576\n",
            "[51200/60000 ( 85%)]  Loss: 0.2406\n",
            "\n",
            "Average test loss: 0.2146  Accuracy: 9362/10000 (93.62%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1085\n",
            "[12800/60000 ( 21%)]  Loss: 0.1843\n",
            "[25600/60000 ( 43%)]  Loss: 0.1536\n",
            "[38400/60000 ( 64%)]  Loss: 0.1311\n",
            "[51200/60000 ( 85%)]  Loss: 0.1128\n",
            "\n",
            "Average test loss: 0.1360  Accuracy: 9567/10000 (95.67%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1703\n",
            "[12800/60000 ( 21%)]  Loss: 0.1683\n",
            "[25600/60000 ( 43%)]  Loss: 0.2574\n",
            "[38400/60000 ( 64%)]  Loss: 0.0948\n",
            "[51200/60000 ( 85%)]  Loss: 0.1614\n",
            "\n",
            "Average test loss: 0.1169  Accuracy: 9622/10000 (96.22%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0932\n",
            "[12800/60000 ( 21%)]  Loss: 0.1381\n",
            "[25600/60000 ( 43%)]  Loss: 0.0532\n",
            "[38400/60000 ( 64%)]  Loss: 0.0978\n",
            "[51200/60000 ( 85%)]  Loss: 0.1658\n",
            "\n",
            "Average test loss: 0.0904  Accuracy: 9721/10000 (97.21%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0483\n",
            "[12800/60000 ( 21%)]  Loss: 0.0271\n",
            "[25600/60000 ( 43%)]  Loss: 0.0776\n",
            "[38400/60000 ( 64%)]  Loss: 0.0939\n",
            "[51200/60000 ( 85%)]  Loss: 0.0543\n",
            "\n",
            "Average test loss: 0.0853  Accuracy: 9746/10000 (97.46%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0574\n",
            "[12800/60000 ( 21%)]  Loss: 0.0400\n",
            "[25600/60000 ( 43%)]  Loss: 0.0824\n",
            "[38400/60000 ( 64%)]  Loss: 0.0585\n",
            "[51200/60000 ( 85%)]  Loss: 0.1135\n",
            "\n",
            "Average test loss: 0.0699  Accuracy: 9776/10000 (97.76%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0248\n",
            "[12800/60000 ( 21%)]  Loss: 0.0632\n",
            "[25600/60000 ( 43%)]  Loss: 0.0272\n",
            "[38400/60000 ( 64%)]  Loss: 0.0333\n",
            "[51200/60000 ( 85%)]  Loss: 0.0532\n",
            "\n",
            "Average test loss: 0.0785  Accuracy: 9764/10000 (97.64%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.1043\n",
            "[12800/60000 ( 21%)]  Loss: 0.0389\n",
            "[25600/60000 ( 43%)]  Loss: 0.0343\n",
            "[38400/60000 ( 64%)]  Loss: 0.0399\n",
            "[51200/60000 ( 85%)]  Loss: 0.0319\n",
            "\n",
            "Average test loss: 0.0734  Accuracy: 9782/10000 (97.82%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0397\n",
            "[12800/60000 ( 21%)]  Loss: 0.0331\n",
            "[25600/60000 ( 43%)]  Loss: 0.0486\n",
            "[38400/60000 ( 64%)]  Loss: 0.0477\n",
            "[51200/60000 ( 85%)]  Loss: 0.0837\n",
            "\n",
            "Average test loss: 0.0581  Accuracy: 9816/10000 (98.16%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0772\n",
            "[12800/60000 ( 21%)]  Loss: 0.0230\n",
            "[25600/60000 ( 43%)]  Loss: 0.0102\n",
            "[38400/60000 ( 64%)]  Loss: 0.0512\n",
            "[51200/60000 ( 85%)]  Loss: 0.0191\n",
            "\n",
            "Average test loss: 0.0732  Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0899\n",
            "[12800/60000 ( 21%)]  Loss: 0.0379\n",
            "[25600/60000 ( 43%)]  Loss: 0.0126\n",
            "[38400/60000 ( 64%)]  Loss: 0.0250\n",
            "[51200/60000 ( 85%)]  Loss: 0.0265\n",
            "\n",
            "Average test loss: 0.0740  Accuracy: 9804/10000 (98.04%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0403\n",
            "[12800/60000 ( 21%)]  Loss: 0.0299\n",
            "[25600/60000 ( 43%)]  Loss: 0.0187\n",
            "[38400/60000 ( 64%)]  Loss: 0.0062\n",
            "[51200/60000 ( 85%)]  Loss: 0.0802\n",
            "\n",
            "Average test loss: 0.0722  Accuracy: 9817/10000 (98.17%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0614\n",
            "[12800/60000 ( 21%)]  Loss: 0.0163\n",
            "[25600/60000 ( 43%)]  Loss: 0.0067\n",
            "[38400/60000 ( 64%)]  Loss: 0.0075\n",
            "[51200/60000 ( 85%)]  Loss: 0.0051\n",
            "\n",
            "Average test loss: 0.0730  Accuracy: 9808/10000 (98.08%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0049\n",
            "[12800/60000 ( 21%)]  Loss: 0.0128\n",
            "[25600/60000 ( 43%)]  Loss: 0.0336\n",
            "[38400/60000 ( 64%)]  Loss: 0.0091\n",
            "[51200/60000 ( 85%)]  Loss: 0.0199\n",
            "\n",
            "Average test loss: 0.0671  Accuracy: 9823/10000 (98.23%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0015\n",
            "[12800/60000 ( 21%)]  Loss: 0.0030\n",
            "[25600/60000 ( 43%)]  Loss: 0.0009\n",
            "[38400/60000 ( 64%)]  Loss: 0.0095\n",
            "[51200/60000 ( 85%)]  Loss: 0.0025\n",
            "\n",
            "Average test loss: 0.0776  Accuracy: 9797/10000 (97.97%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0622\n",
            "[12800/60000 ( 21%)]  Loss: 0.0027\n",
            "[25600/60000 ( 43%)]  Loss: 0.0057\n",
            "[38400/60000 ( 64%)]  Loss: 0.0038\n",
            "[51200/60000 ( 85%)]  Loss: 0.0010\n",
            "\n",
            "Average test loss: 0.0765  Accuracy: 9820/10000 (98.20%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0052\n",
            "[12800/60000 ( 21%)]  Loss: 0.0014\n",
            "[25600/60000 ( 43%)]  Loss: 0.0018\n",
            "[38400/60000 ( 64%)]  Loss: 0.0007\n",
            "[51200/60000 ( 85%)]  Loss: 0.0015\n",
            "\n",
            "Average test loss: 0.0814  Accuracy: 9819/10000 (98.19%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0018\n",
            "[12800/60000 ( 21%)]  Loss: 0.0014\n",
            "[25600/60000 ( 43%)]  Loss: 0.0015\n",
            "[38400/60000 ( 64%)]  Loss: 0.0040\n",
            "[51200/60000 ( 85%)]  Loss: 0.0013\n",
            "\n",
            "Average test loss: 0.0760  Accuracy: 9828/10000 (98.28%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0032\n",
            "[12800/60000 ( 21%)]  Loss: 0.0001\n",
            "[25600/60000 ( 43%)]  Loss: 0.0039\n",
            "[38400/60000 ( 64%)]  Loss: 0.0006\n",
            "[51200/60000 ( 85%)]  Loss: 0.0401\n",
            "\n",
            "Average test loss: 0.0844  Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0027\n",
            "[12800/60000 ( 21%)]  Loss: 0.0049\n",
            "[25600/60000 ( 43%)]  Loss: 0.0054\n",
            "[38400/60000 ( 64%)]  Loss: 0.0283\n",
            "[51200/60000 ( 85%)]  Loss: 0.0063\n",
            "\n",
            "Average test loss: 0.0708  Accuracy: 9838/10000 (98.38%)\n",
            "\n",
            "=== Running ViT variant: smaller ===\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3284\n",
            "[12800/60000 ( 21%)]  Loss: 1.5038\n",
            "[25600/60000 ( 43%)]  Loss: 0.7451\n",
            "[38400/60000 ( 64%)]  Loss: 0.4756\n",
            "[51200/60000 ( 85%)]  Loss: 0.4178\n",
            "\n",
            "Average test loss: 0.2955  Accuracy: 9053/10000 (90.53%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.4084\n",
            "[12800/60000 ( 21%)]  Loss: 0.1717\n",
            "[25600/60000 ( 43%)]  Loss: 0.2159\n",
            "[38400/60000 ( 64%)]  Loss: 0.0768\n",
            "[51200/60000 ( 85%)]  Loss: 0.1394\n",
            "\n",
            "Average test loss: 0.1713  Accuracy: 9468/10000 (94.68%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1054\n",
            "[12800/60000 ( 21%)]  Loss: 0.1211\n",
            "[25600/60000 ( 43%)]  Loss: 0.1652\n",
            "[38400/60000 ( 64%)]  Loss: 0.0850\n",
            "[51200/60000 ( 85%)]  Loss: 0.1473\n",
            "\n",
            "Average test loss: 0.1445  Accuracy: 9522/10000 (95.22%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1158\n",
            "[12800/60000 ( 21%)]  Loss: 0.1026\n",
            "[25600/60000 ( 43%)]  Loss: 0.1549\n",
            "[38400/60000 ( 64%)]  Loss: 0.1840\n",
            "[51200/60000 ( 85%)]  Loss: 0.0382\n",
            "\n",
            "Average test loss: 0.1146  Accuracy: 9633/10000 (96.33%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.1169\n",
            "[12800/60000 ( 21%)]  Loss: 0.1502\n",
            "[25600/60000 ( 43%)]  Loss: 0.1311\n",
            "[38400/60000 ( 64%)]  Loss: 0.2318\n",
            "[51200/60000 ( 85%)]  Loss: 0.1367\n",
            "\n",
            "Average test loss: 0.1046  Accuracy: 9663/10000 (96.63%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.1463\n",
            "[12800/60000 ( 21%)]  Loss: 0.1053\n",
            "[25600/60000 ( 43%)]  Loss: 0.1623\n",
            "[38400/60000 ( 64%)]  Loss: 0.0864\n",
            "[51200/60000 ( 85%)]  Loss: 0.1652\n",
            "\n",
            "Average test loss: 0.1038  Accuracy: 9679/10000 (96.79%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0685\n",
            "[12800/60000 ( 21%)]  Loss: 0.0521\n",
            "[25600/60000 ( 43%)]  Loss: 0.0848\n",
            "[38400/60000 ( 64%)]  Loss: 0.0284\n",
            "[51200/60000 ( 85%)]  Loss: 0.1654\n",
            "\n",
            "Average test loss: 0.0948  Accuracy: 9697/10000 (96.97%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0992\n",
            "[12800/60000 ( 21%)]  Loss: 0.0578\n",
            "[25600/60000 ( 43%)]  Loss: 0.0936\n",
            "[38400/60000 ( 64%)]  Loss: 0.0259\n",
            "[51200/60000 ( 85%)]  Loss: 0.0297\n",
            "\n",
            "Average test loss: 0.0902  Accuracy: 9707/10000 (97.07%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0874\n",
            "[12800/60000 ( 21%)]  Loss: 0.0377\n",
            "[25600/60000 ( 43%)]  Loss: 0.0598\n",
            "[38400/60000 ( 64%)]  Loss: 0.0590\n",
            "[51200/60000 ( 85%)]  Loss: 0.0894\n",
            "\n",
            "Average test loss: 0.0837  Accuracy: 9731/10000 (97.31%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0561\n",
            "[12800/60000 ( 21%)]  Loss: 0.0125\n",
            "[25600/60000 ( 43%)]  Loss: 0.0367\n",
            "[38400/60000 ( 64%)]  Loss: 0.0463\n",
            "[51200/60000 ( 85%)]  Loss: 0.0439\n",
            "\n",
            "Average test loss: 0.0801  Accuracy: 9745/10000 (97.45%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0307\n",
            "[12800/60000 ( 21%)]  Loss: 0.1050\n",
            "[25600/60000 ( 43%)]  Loss: 0.0343\n",
            "[38400/60000 ( 64%)]  Loss: 0.0424\n",
            "[51200/60000 ( 85%)]  Loss: 0.0642\n",
            "\n",
            "Average test loss: 0.0859  Accuracy: 9755/10000 (97.55%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0314\n",
            "[12800/60000 ( 21%)]  Loss: 0.0127\n",
            "[25600/60000 ( 43%)]  Loss: 0.0897\n",
            "[38400/60000 ( 64%)]  Loss: 0.0370\n",
            "[51200/60000 ( 85%)]  Loss: 0.0140\n",
            "\n",
            "Average test loss: 0.0775  Accuracy: 9773/10000 (97.73%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0284\n",
            "[12800/60000 ( 21%)]  Loss: 0.0475\n",
            "[25600/60000 ( 43%)]  Loss: 0.0684\n",
            "[38400/60000 ( 64%)]  Loss: 0.0517\n",
            "[51200/60000 ( 85%)]  Loss: 0.0189\n",
            "\n",
            "Average test loss: 0.0825  Accuracy: 9744/10000 (97.44%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0150\n",
            "[12800/60000 ( 21%)]  Loss: 0.0610\n",
            "[25600/60000 ( 43%)]  Loss: 0.0398\n",
            "[38400/60000 ( 64%)]  Loss: 0.0068\n",
            "[51200/60000 ( 85%)]  Loss: 0.0533\n",
            "\n",
            "Average test loss: 0.0811  Accuracy: 9749/10000 (97.49%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0568\n",
            "[12800/60000 ( 21%)]  Loss: 0.0453\n",
            "[25600/60000 ( 43%)]  Loss: 0.0182\n",
            "[38400/60000 ( 64%)]  Loss: 0.0839\n",
            "[51200/60000 ( 85%)]  Loss: 0.0608\n",
            "\n",
            "Average test loss: 0.0852  Accuracy: 9748/10000 (97.48%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0786\n",
            "[12800/60000 ( 21%)]  Loss: 0.0463\n",
            "[25600/60000 ( 43%)]  Loss: 0.0149\n",
            "[38400/60000 ( 64%)]  Loss: 0.0666\n",
            "[51200/60000 ( 85%)]  Loss: 0.0141\n",
            "\n",
            "Average test loss: 0.0761  Accuracy: 9781/10000 (97.81%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0099\n",
            "[12800/60000 ( 21%)]  Loss: 0.0464\n",
            "[25600/60000 ( 43%)]  Loss: 0.0393\n",
            "[38400/60000 ( 64%)]  Loss: 0.0113\n",
            "[51200/60000 ( 85%)]  Loss: 0.0167\n",
            "\n",
            "Average test loss: 0.0798  Accuracy: 9782/10000 (97.82%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0157\n",
            "[12800/60000 ( 21%)]  Loss: 0.0494\n",
            "[25600/60000 ( 43%)]  Loss: 0.0494\n",
            "[38400/60000 ( 64%)]  Loss: 0.0307\n",
            "[51200/60000 ( 85%)]  Loss: 0.0097\n",
            "\n",
            "Average test loss: 0.0797  Accuracy: 9790/10000 (97.90%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0120\n",
            "[12800/60000 ( 21%)]  Loss: 0.0111\n",
            "[25600/60000 ( 43%)]  Loss: 0.0264\n",
            "[38400/60000 ( 64%)]  Loss: 0.0441\n",
            "[51200/60000 ( 85%)]  Loss: 0.0502\n",
            "\n",
            "Average test loss: 0.0791  Accuracy: 9786/10000 (97.86%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0802\n",
            "[12800/60000 ( 21%)]  Loss: 0.0091\n",
            "[25600/60000 ( 43%)]  Loss: 0.0044\n",
            "[38400/60000 ( 64%)]  Loss: 0.0387\n",
            "[51200/60000 ( 85%)]  Loss: 0.0335\n",
            "\n",
            "Average test loss: 0.0844  Accuracy: 9766/10000 (97.66%)\n",
            "\n",
            "=== Running ViT variant: wider ===\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3279\n",
            "[12800/60000 ( 21%)]  Loss: 0.4768\n",
            "[25600/60000 ( 43%)]  Loss: 0.3719\n",
            "[38400/60000 ( 64%)]  Loss: 0.2031\n",
            "[51200/60000 ( 85%)]  Loss: 0.1757\n",
            "\n",
            "Average test loss: 0.1771  Accuracy: 9445/10000 (94.45%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1243\n",
            "[12800/60000 ( 21%)]  Loss: 0.0473\n",
            "[25600/60000 ( 43%)]  Loss: 0.0559\n",
            "[38400/60000 ( 64%)]  Loss: 0.0547\n",
            "[51200/60000 ( 85%)]  Loss: 0.1297\n",
            "\n",
            "Average test loss: 0.0999  Accuracy: 9686/10000 (96.86%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0769\n",
            "[12800/60000 ( 21%)]  Loss: 0.2035\n",
            "[25600/60000 ( 43%)]  Loss: 0.1001\n",
            "[38400/60000 ( 64%)]  Loss: 0.0550\n",
            "[51200/60000 ( 85%)]  Loss: 0.1576\n",
            "\n",
            "Average test loss: 0.0859  Accuracy: 9732/10000 (97.32%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0182\n",
            "[12800/60000 ( 21%)]  Loss: 0.0774\n",
            "[25600/60000 ( 43%)]  Loss: 0.1210\n",
            "[38400/60000 ( 64%)]  Loss: 0.0386\n",
            "[51200/60000 ( 85%)]  Loss: 0.0504\n",
            "\n",
            "Average test loss: 0.0822  Accuracy: 9765/10000 (97.65%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0339\n",
            "[12800/60000 ( 21%)]  Loss: 0.0298\n",
            "[25600/60000 ( 43%)]  Loss: 0.0668\n",
            "[38400/60000 ( 64%)]  Loss: 0.0078\n",
            "[51200/60000 ( 85%)]  Loss: 0.0442\n",
            "\n",
            "Average test loss: 0.0825  Accuracy: 9745/10000 (97.45%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0919\n",
            "[12800/60000 ( 21%)]  Loss: 0.0689\n",
            "[25600/60000 ( 43%)]  Loss: 0.0765\n",
            "[38400/60000 ( 64%)]  Loss: 0.0657\n",
            "[51200/60000 ( 85%)]  Loss: 0.0269\n",
            "\n",
            "Average test loss: 0.0711  Accuracy: 9788/10000 (97.88%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0665\n",
            "[12800/60000 ( 21%)]  Loss: 0.0124\n",
            "[25600/60000 ( 43%)]  Loss: 0.0148\n",
            "[38400/60000 ( 64%)]  Loss: 0.0113\n",
            "[51200/60000 ( 85%)]  Loss: 0.0376\n",
            "\n",
            "Average test loss: 0.0676  Accuracy: 9806/10000 (98.06%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0274\n",
            "[12800/60000 ( 21%)]  Loss: 0.0241\n",
            "[25600/60000 ( 43%)]  Loss: 0.0074\n",
            "[38400/60000 ( 64%)]  Loss: 0.0076\n",
            "[51200/60000 ( 85%)]  Loss: 0.0143\n",
            "\n",
            "Average test loss: 0.0709  Accuracy: 9804/10000 (98.04%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0702\n",
            "[12800/60000 ( 21%)]  Loss: 0.0179\n",
            "[25600/60000 ( 43%)]  Loss: 0.0231\n",
            "[38400/60000 ( 64%)]  Loss: 0.0065\n",
            "[51200/60000 ( 85%)]  Loss: 0.0065\n",
            "\n",
            "Average test loss: 0.0628  Accuracy: 9822/10000 (98.22%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0017\n",
            "[12800/60000 ( 21%)]  Loss: 0.0026\n",
            "[25600/60000 ( 43%)]  Loss: 0.0290\n",
            "[38400/60000 ( 64%)]  Loss: 0.0153\n",
            "[51200/60000 ( 85%)]  Loss: 0.0020\n",
            "\n",
            "Average test loss: 0.0758  Accuracy: 9801/10000 (98.01%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0132\n",
            "[12800/60000 ( 21%)]  Loss: 0.0228\n",
            "[25600/60000 ( 43%)]  Loss: 0.0403\n",
            "[38400/60000 ( 64%)]  Loss: 0.0035\n",
            "[51200/60000 ( 85%)]  Loss: 0.0459\n",
            "\n",
            "Average test loss: 0.0749  Accuracy: 9804/10000 (98.04%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0021\n",
            "[12800/60000 ( 21%)]  Loss: 0.0097\n",
            "[25600/60000 ( 43%)]  Loss: 0.0018\n",
            "[38400/60000 ( 64%)]  Loss: 0.0104\n",
            "[51200/60000 ( 85%)]  Loss: 0.0194\n",
            "\n",
            "Average test loss: 0.0786  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0237\n",
            "[12800/60000 ( 21%)]  Loss: 0.0409\n",
            "[25600/60000 ( 43%)]  Loss: 0.0553\n",
            "[38400/60000 ( 64%)]  Loss: 0.0169\n",
            "[51200/60000 ( 85%)]  Loss: 0.0066\n",
            "\n",
            "Average test loss: 0.0636  Accuracy: 9843/10000 (98.43%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0006\n",
            "[12800/60000 ( 21%)]  Loss: 0.0012\n",
            "[25600/60000 ( 43%)]  Loss: 0.0239\n",
            "[38400/60000 ( 64%)]  Loss: 0.0017\n",
            "[51200/60000 ( 85%)]  Loss: 0.0028\n",
            "\n",
            "Average test loss: 0.0637  Accuracy: 9849/10000 (98.49%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0347\n",
            "[12800/60000 ( 21%)]  Loss: 0.0033\n",
            "[25600/60000 ( 43%)]  Loss: 0.0008\n",
            "[38400/60000 ( 64%)]  Loss: 0.0090\n",
            "[51200/60000 ( 85%)]  Loss: 0.0005\n",
            "\n",
            "Average test loss: 0.0803  Accuracy: 9814/10000 (98.14%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0033\n",
            "[12800/60000 ( 21%)]  Loss: 0.0002\n",
            "[25600/60000 ( 43%)]  Loss: 0.0144\n",
            "[38400/60000 ( 64%)]  Loss: 0.0009\n",
            "[51200/60000 ( 85%)]  Loss: 0.0042\n",
            "\n",
            "Average test loss: 0.0721  Accuracy: 9825/10000 (98.25%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0021\n",
            "[12800/60000 ( 21%)]  Loss: 0.0206\n",
            "[25600/60000 ( 43%)]  Loss: 0.0124\n",
            "[38400/60000 ( 64%)]  Loss: 0.0106\n",
            "[51200/60000 ( 85%)]  Loss: 0.0006\n",
            "\n",
            "Average test loss: 0.0582  Accuracy: 9866/10000 (98.66%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0025\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0130\n",
            "[38400/60000 ( 64%)]  Loss: 0.0010\n",
            "[51200/60000 ( 85%)]  Loss: 0.0004\n",
            "\n",
            "Average test loss: 0.0580  Accuracy: 9866/10000 (98.66%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0124\n",
            "[12800/60000 ( 21%)]  Loss: 0.0115\n",
            "[25600/60000 ( 43%)]  Loss: 0.0014\n",
            "[38400/60000 ( 64%)]  Loss: 0.0006\n",
            "[51200/60000 ( 85%)]  Loss: 0.0036\n",
            "\n",
            "Average test loss: 0.0674  Accuracy: 9863/10000 (98.63%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0017\n",
            "[12800/60000 ( 21%)]  Loss: 0.0050\n",
            "[25600/60000 ( 43%)]  Loss: 0.0247\n",
            "[38400/60000 ( 64%)]  Loss: 0.0163\n",
            "[51200/60000 ( 85%)]  Loss: 0.0007\n",
            "\n",
            "Average test loss: 0.0776  Accuracy: 9844/10000 (98.44%)\n",
            "\n",
            "=== Running ViT variant: deeper ===\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3743\n",
            "[12800/60000 ( 21%)]  Loss: 1.2359\n",
            "[25600/60000 ( 43%)]  Loss: 0.4804\n",
            "[38400/60000 ( 64%)]  Loss: 0.3580\n",
            "[51200/60000 ( 85%)]  Loss: 0.2277\n",
            "\n",
            "Average test loss: 0.2219  Accuracy: 9279/10000 (92.79%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.2378\n",
            "[12800/60000 ( 21%)]  Loss: 0.1006\n",
            "[25600/60000 ( 43%)]  Loss: 0.2464\n",
            "[38400/60000 ( 64%)]  Loss: 0.2087\n",
            "[51200/60000 ( 85%)]  Loss: 0.1594\n",
            "\n",
            "Average test loss: 0.1313  Accuracy: 9605/10000 (96.05%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1031\n",
            "[12800/60000 ( 21%)]  Loss: 0.1004\n",
            "[25600/60000 ( 43%)]  Loss: 0.0683\n",
            "[38400/60000 ( 64%)]  Loss: 0.1120\n",
            "[51200/60000 ( 85%)]  Loss: 0.0952\n",
            "\n",
            "Average test loss: 0.1037  Accuracy: 9678/10000 (96.78%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.1366\n",
            "[12800/60000 ( 21%)]  Loss: 0.0790\n",
            "[25600/60000 ( 43%)]  Loss: 0.0790\n",
            "[38400/60000 ( 64%)]  Loss: 0.0591\n",
            "[51200/60000 ( 85%)]  Loss: 0.1041\n",
            "\n",
            "Average test loss: 0.0954  Accuracy: 9711/10000 (97.11%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0402\n",
            "[12800/60000 ( 21%)]  Loss: 0.1174\n",
            "[25600/60000 ( 43%)]  Loss: 0.0192\n",
            "[38400/60000 ( 64%)]  Loss: 0.0747\n",
            "[51200/60000 ( 85%)]  Loss: 0.0468\n",
            "\n",
            "Average test loss: 0.0730  Accuracy: 9779/10000 (97.79%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0996\n",
            "[12800/60000 ( 21%)]  Loss: 0.0403\n",
            "[25600/60000 ( 43%)]  Loss: 0.0331\n",
            "[38400/60000 ( 64%)]  Loss: 0.0795\n",
            "[51200/60000 ( 85%)]  Loss: 0.0476\n",
            "\n",
            "Average test loss: 0.0789  Accuracy: 9762/10000 (97.62%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0333\n",
            "[12800/60000 ( 21%)]  Loss: 0.0359\n",
            "[25600/60000 ( 43%)]  Loss: 0.0742\n",
            "[38400/60000 ( 64%)]  Loss: 0.0227\n",
            "[51200/60000 ( 85%)]  Loss: 0.0123\n",
            "\n",
            "Average test loss: 0.0730  Accuracy: 9788/10000 (97.88%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0816\n",
            "[12800/60000 ( 21%)]  Loss: 0.0292\n",
            "[25600/60000 ( 43%)]  Loss: 0.0426\n",
            "[38400/60000 ( 64%)]  Loss: 0.0676\n",
            "[51200/60000 ( 85%)]  Loss: 0.0227\n",
            "\n",
            "Average test loss: 0.0750  Accuracy: 9779/10000 (97.79%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0293\n",
            "[12800/60000 ( 21%)]  Loss: 0.0710\n",
            "[25600/60000 ( 43%)]  Loss: 0.0149\n",
            "[38400/60000 ( 64%)]  Loss: 0.0463\n",
            "[51200/60000 ( 85%)]  Loss: 0.0245\n",
            "\n",
            "Average test loss: 0.0671  Accuracy: 9809/10000 (98.09%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0874\n",
            "[12800/60000 ( 21%)]  Loss: 0.0438\n",
            "[25600/60000 ( 43%)]  Loss: 0.0223\n",
            "[38400/60000 ( 64%)]  Loss: 0.0210\n",
            "[51200/60000 ( 85%)]  Loss: 0.0040\n",
            "\n",
            "Average test loss: 0.0649  Accuracy: 9814/10000 (98.14%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0069\n",
            "[12800/60000 ( 21%)]  Loss: 0.0063\n",
            "[25600/60000 ( 43%)]  Loss: 0.0034\n",
            "[38400/60000 ( 64%)]  Loss: 0.0383\n",
            "[51200/60000 ( 85%)]  Loss: 0.0181\n",
            "\n",
            "Average test loss: 0.0668  Accuracy: 9819/10000 (98.19%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0280\n",
            "[12800/60000 ( 21%)]  Loss: 0.0066\n",
            "[25600/60000 ( 43%)]  Loss: 0.0467\n",
            "[38400/60000 ( 64%)]  Loss: 0.0076\n",
            "[51200/60000 ( 85%)]  Loss: 0.0258\n",
            "\n",
            "Average test loss: 0.0581  Accuracy: 9842/10000 (98.42%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0091\n",
            "[12800/60000 ( 21%)]  Loss: 0.0306\n",
            "[25600/60000 ( 43%)]  Loss: 0.0162\n",
            "[38400/60000 ( 64%)]  Loss: 0.0145\n",
            "[51200/60000 ( 85%)]  Loss: 0.0730\n",
            "\n",
            "Average test loss: 0.0713  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0143\n",
            "[12800/60000 ( 21%)]  Loss: 0.0134\n",
            "[25600/60000 ( 43%)]  Loss: 0.0069\n",
            "[38400/60000 ( 64%)]  Loss: 0.0059\n",
            "[51200/60000 ( 85%)]  Loss: 0.0314\n",
            "\n",
            "Average test loss: 0.0685  Accuracy: 9827/10000 (98.27%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0544\n",
            "[12800/60000 ( 21%)]  Loss: 0.0064\n",
            "[25600/60000 ( 43%)]  Loss: 0.0083\n",
            "[38400/60000 ( 64%)]  Loss: 0.0405\n",
            "[51200/60000 ( 85%)]  Loss: 0.0378\n",
            "\n",
            "Average test loss: 0.0924  Accuracy: 9766/10000 (97.66%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0879\n",
            "[12800/60000 ( 21%)]  Loss: 0.0017\n",
            "[25600/60000 ( 43%)]  Loss: 0.0056\n",
            "[38400/60000 ( 64%)]  Loss: 0.0027\n",
            "[51200/60000 ( 85%)]  Loss: 0.0610\n",
            "\n",
            "Average test loss: 0.0655  Accuracy: 9832/10000 (98.32%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0171\n",
            "[12800/60000 ( 21%)]  Loss: 0.0025\n",
            "[25600/60000 ( 43%)]  Loss: 0.0180\n",
            "[38400/60000 ( 64%)]  Loss: 0.0317\n",
            "[51200/60000 ( 85%)]  Loss: 0.0015\n",
            "\n",
            "Average test loss: 0.0629  Accuracy: 9848/10000 (98.48%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0008\n",
            "[12800/60000 ( 21%)]  Loss: 0.0008\n",
            "[25600/60000 ( 43%)]  Loss: 0.0010\n",
            "[38400/60000 ( 64%)]  Loss: 0.0003\n",
            "[51200/60000 ( 85%)]  Loss: 0.0245\n",
            "\n",
            "Average test loss: 0.0848  Accuracy: 9813/10000 (98.13%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0192\n",
            "[12800/60000 ( 21%)]  Loss: 0.0006\n",
            "[25600/60000 ( 43%)]  Loss: 0.0033\n",
            "[38400/60000 ( 64%)]  Loss: 0.0015\n",
            "[51200/60000 ( 85%)]  Loss: 0.0003\n",
            "\n",
            "Average test loss: 0.0705  Accuracy: 9848/10000 (98.48%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0009\n",
            "[12800/60000 ( 21%)]  Loss: 0.0073\n",
            "[25600/60000 ( 43%)]  Loss: 0.0117\n",
            "[38400/60000 ( 64%)]  Loss: 0.0219\n",
            "[51200/60000 ( 85%)]  Loss: 0.0078\n",
            "\n",
            "Average test loss: 0.0678  Accuracy: 9840/10000 (98.40%)\n",
            "\n",
            "=== Running ViT variant: combo ===\n",
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.2994\n",
            "[12800/60000 ( 21%)]  Loss: 0.8759\n",
            "[25600/60000 ( 43%)]  Loss: 0.2997\n",
            "[38400/60000 ( 64%)]  Loss: 0.2695\n",
            "[51200/60000 ( 85%)]  Loss: 0.2279\n",
            "\n",
            "Average test loss: 0.1632  Accuracy: 9499/10000 (94.99%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.1882\n",
            "[12800/60000 ( 21%)]  Loss: 0.1806\n",
            "[25600/60000 ( 43%)]  Loss: 0.1943\n",
            "[38400/60000 ( 64%)]  Loss: 0.0999\n",
            "[51200/60000 ( 85%)]  Loss: 0.1723\n",
            "\n",
            "Average test loss: 0.1152  Accuracy: 9631/10000 (96.31%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.0651\n",
            "[12800/60000 ( 21%)]  Loss: 0.1707\n",
            "[25600/60000 ( 43%)]  Loss: 0.0967\n",
            "[38400/60000 ( 64%)]  Loss: 0.0784\n",
            "[51200/60000 ( 85%)]  Loss: 0.0718\n",
            "\n",
            "Average test loss: 0.1197  Accuracy: 9610/10000 (96.10%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0689\n",
            "[12800/60000 ( 21%)]  Loss: 0.0340\n",
            "[25600/60000 ( 43%)]  Loss: 0.1551\n",
            "[38400/60000 ( 64%)]  Loss: 0.0120\n",
            "[51200/60000 ( 85%)]  Loss: 0.0432\n",
            "\n",
            "Average test loss: 0.0736  Accuracy: 9777/10000 (97.77%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0834\n",
            "[12800/60000 ( 21%)]  Loss: 0.0324\n",
            "[25600/60000 ( 43%)]  Loss: 0.0359\n",
            "[38400/60000 ( 64%)]  Loss: 0.1483\n",
            "[51200/60000 ( 85%)]  Loss: 0.0615\n",
            "\n",
            "Average test loss: 0.1045  Accuracy: 9692/10000 (96.92%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0847\n",
            "[12800/60000 ( 21%)]  Loss: 0.0308\n",
            "[25600/60000 ( 43%)]  Loss: 0.0429\n",
            "[38400/60000 ( 64%)]  Loss: 0.0731\n",
            "[51200/60000 ( 85%)]  Loss: 0.0257\n",
            "\n",
            "Average test loss: 0.0726  Accuracy: 9786/10000 (97.86%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.0477\n",
            "[12800/60000 ( 21%)]  Loss: 0.0050\n",
            "[25600/60000 ( 43%)]  Loss: 0.0253\n",
            "[38400/60000 ( 64%)]  Loss: 0.0313\n",
            "[51200/60000 ( 85%)]  Loss: 0.0479\n",
            "\n",
            "Average test loss: 0.0632  Accuracy: 9803/10000 (98.03%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.0363\n",
            "[12800/60000 ( 21%)]  Loss: 0.0128\n",
            "[25600/60000 ( 43%)]  Loss: 0.0461\n",
            "[38400/60000 ( 64%)]  Loss: 0.0056\n",
            "[51200/60000 ( 85%)]  Loss: 0.0623\n",
            "\n",
            "Average test loss: 0.0719  Accuracy: 9800/10000 (98.00%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0265\n",
            "[12800/60000 ( 21%)]  Loss: 0.0610\n",
            "[25600/60000 ( 43%)]  Loss: 0.0393\n",
            "[38400/60000 ( 64%)]  Loss: 0.0101\n",
            "[51200/60000 ( 85%)]  Loss: 0.0387\n",
            "\n",
            "Average test loss: 0.0628  Accuracy: 9815/10000 (98.15%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0286\n",
            "[12800/60000 ( 21%)]  Loss: 0.0172\n",
            "[25600/60000 ( 43%)]  Loss: 0.0040\n",
            "[38400/60000 ( 64%)]  Loss: 0.0731\n",
            "[51200/60000 ( 85%)]  Loss: 0.0051\n",
            "\n",
            "Average test loss: 0.0612  Accuracy: 9842/10000 (98.42%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0022\n",
            "[12800/60000 ( 21%)]  Loss: 0.0060\n",
            "[25600/60000 ( 43%)]  Loss: 0.0118\n",
            "[38400/60000 ( 64%)]  Loss: 0.0418\n",
            "[51200/60000 ( 85%)]  Loss: 0.0138\n",
            "\n",
            "Average test loss: 0.0537  Accuracy: 9845/10000 (98.45%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0050\n",
            "[12800/60000 ( 21%)]  Loss: 0.0496\n",
            "[25600/60000 ( 43%)]  Loss: 0.0021\n",
            "[38400/60000 ( 64%)]  Loss: 0.1219\n",
            "[51200/60000 ( 85%)]  Loss: 0.0143\n",
            "\n",
            "Average test loss: 0.0740  Accuracy: 9827/10000 (98.27%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0131\n",
            "[12800/60000 ( 21%)]  Loss: 0.0208\n",
            "[25600/60000 ( 43%)]  Loss: 0.0290\n",
            "[38400/60000 ( 64%)]  Loss: 0.0218\n",
            "[51200/60000 ( 85%)]  Loss: 0.0092\n",
            "\n",
            "Average test loss: 0.0690  Accuracy: 9816/10000 (98.16%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0010\n",
            "[12800/60000 ( 21%)]  Loss: 0.0397\n",
            "[25600/60000 ( 43%)]  Loss: 0.0290\n",
            "[38400/60000 ( 64%)]  Loss: 0.0009\n",
            "[51200/60000 ( 85%)]  Loss: 0.0016\n",
            "\n",
            "Average test loss: 0.0946  Accuracy: 9812/10000 (98.12%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0316\n",
            "[12800/60000 ( 21%)]  Loss: 0.0019\n",
            "[25600/60000 ( 43%)]  Loss: 0.0007\n",
            "[38400/60000 ( 64%)]  Loss: 0.0105\n",
            "[51200/60000 ( 85%)]  Loss: 0.0245\n",
            "\n",
            "Average test loss: 0.0702  Accuracy: 9831/10000 (98.31%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0050\n",
            "[12800/60000 ( 21%)]  Loss: 0.0147\n",
            "[25600/60000 ( 43%)]  Loss: 0.0047\n",
            "[38400/60000 ( 64%)]  Loss: 0.0026\n",
            "[51200/60000 ( 85%)]  Loss: 0.0062\n",
            "\n",
            "Average test loss: 0.0705  Accuracy: 9859/10000 (98.59%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0008\n",
            "[12800/60000 ( 21%)]  Loss: 0.0039\n",
            "[25600/60000 ( 43%)]  Loss: 0.0002\n",
            "[38400/60000 ( 64%)]  Loss: 0.0096\n",
            "[51200/60000 ( 85%)]  Loss: 0.0012\n",
            "\n",
            "Average test loss: 0.0749  Accuracy: 9844/10000 (98.44%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0154\n",
            "[12800/60000 ( 21%)]  Loss: 0.0001\n",
            "[25600/60000 ( 43%)]  Loss: 0.0390\n",
            "[38400/60000 ( 64%)]  Loss: 0.0017\n",
            "[51200/60000 ( 85%)]  Loss: 0.0191\n",
            "\n",
            "Average test loss: 0.0690  Accuracy: 9855/10000 (98.55%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0244\n",
            "[12800/60000 ( 21%)]  Loss: 0.0310\n",
            "[25600/60000 ( 43%)]  Loss: 0.0002\n",
            "[38400/60000 ( 64%)]  Loss: 0.0357\n",
            "[51200/60000 ( 85%)]  Loss: 0.0002\n",
            "\n",
            "Average test loss: 0.0669  Accuracy: 9860/10000 (98.60%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0136\n",
            "[12800/60000 ( 21%)]  Loss: 0.0002\n",
            "[25600/60000 ( 43%)]  Loss: 0.0009\n",
            "[38400/60000 ( 64%)]  Loss: 0.0056\n",
            "[51200/60000 ( 85%)]  Loss: 0.0001\n",
            "\n",
            "Average test loss: 0.0703  Accuracy: 9858/10000 (98.58%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XicoRf8_nUTK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}